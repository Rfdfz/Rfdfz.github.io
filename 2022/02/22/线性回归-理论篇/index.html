<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.0.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/icons8-pixel-star-96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/icons8-pixel-star-48.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-flash.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">

  <meta name="description" content="线性回归模型中的梯度下降法">
<meta property="og:type" content="article">
<meta property="og:title" content="线性回归-理论篇">
<meta property="og:url" content="http://example.com/2022/02/22/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E7%90%86%E8%AE%BA%E7%AF%87/index.html">
<meta property="og:site_name" content="Please Lamp The Star">
<meta property="og:description" content="线性回归模型中的梯度下降法">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220223180126139.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220224212030053.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220224212639223.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220224213434009.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220306181054942.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220306181540173.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220309100414308.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220310140929602.png">
<meta property="article:published_time" content="2022-02-22T12:10:34.000Z">
<meta property="article:modified_time" content="2022-04-21T16:29:24.192Z">
<meta property="article:author" content="林程星">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220223180126139.png">

<link rel="canonical" href="http://example.com/2022/02/22/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E7%90%86%E8%AE%BA%E7%AF%87/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>线性回归-理论篇 | Please Lamp The Star</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Please Lamp The Star</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/02/22/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E7%90%86%E8%AE%BA%E7%AF%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="林程星">
      <meta itemprop="description" content="‘绳’和‘棒’一样，是人类最早创造的工具之一。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Please Lamp The Star">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          线性回归-理论篇
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-22 20:10:34" itemprop="dateCreated datePublished" datetime="2022-02-22T20:10:34+08:00">2022-02-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-04-22 00:29:24" itemprop="dateModified" datetime="2022-04-22T00:29:24+08:00">2022-04-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>6.8k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>6 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>线性回归模型中的梯度下降法</p>
<span id="more"></span>
<h1 id="线性回归-理论篇"><a href="#线性回归-理论篇" class="headerlink" title="线性回归-理论篇"></a>线性回归-理论篇</h1><p>线性回归模型属于监督学习中的回归问题</p>
<p>在监督学习中，我们有一个数据集，它被称为<strong>训练集(Training Set)</strong></p>
<h2 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h2><h3 id="监督学习的工作原理"><a href="#监督学习的工作原理" class="headerlink" title="监督学习的工作原理"></a>监督学习的工作原理</h3><ol>
<li><p>先向学习算法提供一个训练集</p>
</li>
<li><p>算法根据训练集输出一个<strong>假设函数h(hypothesis)</strong></p>
</li>
<li><p>然后数据的特征值根据假设函数计算出相应的预测值</p>
</li>
<li><p>决定一个假设函数，例如：</p>
<script type="math/tex; mode=display">
h_\theta(x)=\theta_0+\theta_1x</script></li>
</ol>
<p>其中<script type="math/tex">\theta_i</script>被称为<strong>模型参数(Parameters)</strong></p>
<h3 id="代价函数-cost-function"><a href="#代价函数-cost-function" class="headerlink" title="代价函数(cost function)"></a>代价函数(cost function)</h3><h4 id="代价函数的数学定义"><a href="#代价函数的数学定义" class="headerlink" title="代价函数的数学定义"></a>代价函数的数学定义</h4><p>我们要如何使得假设函数能够更好的拟合数据呢？我们要尽可能选择合适的<script type="math/tex">\theta_0</script>和<script type="math/tex">\theta_1</script>使得对于假设函数<script type="math/tex">h_\theta(x)</script>从训练集输入x时所得到的值尽可能接近该样本对应的y值，即<script type="math/tex">h_\theta(x)-y</script>取得极小值</p>
<p>这样就得到预测值和实际值的差的平方误差和公式</p>
<script type="math/tex; mode=display">
\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2</script><p>其中<script type="math/tex">m</script>为训练集的样本容量，<script type="math/tex">x^{(i)}</script>与<script type="math/tex">y^{(i)}</script>中的<script type="math/tex">i</script>为序号</p>
<p>我们要求的是使得上述式子成立的<script type="math/tex">\theta_0</script>和<script type="math/tex">\theta_1</script></p>
<p>由此我们定义一个<strong>代价函数(cost function)</strong></p>
<script type="math/tex; mode=display">
J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2</script><p>这种代价函数也被称为<strong>平方误差代价函数(squared error cost function)</strong></p>
<p>我们的优化目标是</p>
<script type="math/tex; mode=display">
\mathop{minimize}\limits_{\theta_0,\theta_1}J(\theta_0,\theta_1)</script><p>还有其他的代价函数也能很好的发挥作用，但是但是平方误差代价函数可能是解决回归问题的最常用手段</p>
<h4 id="如何理解代价函数"><a href="#如何理解代价函数" class="headerlink" title="如何理解代价函数"></a>如何理解代价函数</h4><p>为了更好的使代价函数J可视化，</p>
<p>我们使用一个简化版的假设函数<script type="math/tex">h_\theta(x)=\theta_1x</script></p>
<p>此时的代价函数为<script type="math/tex">J(\theta_1)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2</script></p>
<p>此时我们的优化目标为<script type="math/tex">\mathop{minimize}\limits_{\theta_1}J(\theta_1)</script></p>
<p>先来看看假设函数，若给定参数<script type="math/tex">\theta_1</script>则假设函数是一个关于x的函数，若将函数绘制出来则是一条经过原点的直线</p>
<p>而代价函数是一个关于<script type="math/tex">\theta_1</script>的函数，因此每一个不同的<script type="math/tex">\theta_1</script>都对应着不同的假设函数，由此可以绘制出代价函数的图像</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220223180126139.png" alt="image-20220223180126139"></p>
<p>还记得我们学习算法的优化目标吗，是找到通过选择<script type="math/tex">\theta_1</script>找到<script type="math/tex">J(\theta_1)</script>的最小值，对于上图这个特殊的训练集<script type="math/tex">(1,1)(2,2)(3,3)</script>来说最小值<script type="math/tex">\theta_1=1</script>已经完美拟合了它</p>
<h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>梯度下降法将用于最小化不同的函数，它不止在线性回归中用到，还会在其它机器学习领域用到</p>
<p>我们有一个函数<script type="math/tex">J(\theta_0,\theta_1)</script>我们要用梯度下降法来求<script type="math/tex">\mathop{min}\limits_{\theta_0,\theta_1}J(\theta_0,\theta_1)</script></p>
<p>思路：一般先设<script type="math/tex">\theta_0=0，\theta_1=0</script>，接下来不停的一点点改变<script type="math/tex">\theta_0，\theta_1</script>使得<script type="math/tex">J(\theta_0,\theta_1)</script>变小，直到我们找到<script type="math/tex">J(\theta_0,\theta_1)</script>的最小值或局部最小值</p>
<h4 id="梯度下降算法的数学定义"><a href="#梯度下降算法的数学定义" class="headerlink" title="梯度下降算法的数学定义"></a>梯度下降算法的数学定义</h4><script type="math/tex; mode=display">
\rm{repeat}\;\rm{until}\;\rm{convergence}\;\{\\\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\;\;\;(\rm{for}\;j=0\;\rm{and}\;j=1)\;\}</script><ul>
<li><script type="math/tex">:=</script>为计算机中的赋值操作</li>
<li><script type="math/tex">\alpha</script>为<strong>学习率(learning rate)</strong>，用于控制梯度下降时的速度</li>
</ul>
<h4 id="同步更新"><a href="#同步更新" class="headerlink" title="同步更新"></a>同步更新</h4><p><strong>正确的实现方法</strong></p>
<ol>
<li><script type="math/tex; mode=display">\rm{temp0}:=\theta_0-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)</script></li>
<li><script type="math/tex; mode=display">\rm{temp1}:=\theta_1-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)</script></li>
<li><script type="math/tex; mode=display">\theta_0:=\rm{temp0}</script></li>
<li><script type="math/tex; mode=display">\theta_1:=\rm{temp1}</script></li>
</ol>
<p><strong>错误的实现方法(没有做到同步更新)</strong></p>
<ol>
<li><script type="math/tex; mode=display">\rm{temp0}:=\theta_0-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)</script></li>
<li><script type="math/tex; mode=display">\theta_0:=\rm{temp0}</script></li>
<li><script type="math/tex; mode=display">\rm{temp1}:=\theta_1-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)</script></li>
<li><script type="math/tex; mode=display">\theta_1:=\rm{temp1}</script></li>
</ol>
<p>如果这样先执行第2步，则第三步中的导数项将会使用新的<script type="math/tex">\theta_0</script>来计算</p>
<h4 id="如何理解梯度下降法"><a href="#如何理解梯度下降法" class="headerlink" title="如何理解梯度下降法"></a>如何理解梯度下降法</h4><p>先来看看导数部分，假设我们有个简化版的代价函数<script type="math/tex">J(\theta_1)</script>，此时梯度下降法中的导数项将变为<script type="math/tex">\alpha\frac{d}{d\theta_1}J(\theta_1)</script>，其中的<script type="math/tex">\alpha</script>永远是正数，若<script type="math/tex">J(\theta_1)</script>有正导数，则<script type="math/tex">\theta_1</script>将不断变小，若<script type="math/tex">J(\theta_1)</script>有负导数，则<script type="math/tex">\theta_1</script>将不断变大，得到的结果是函数值不断逼近极小值</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220224212030053.png" alt=""></p>
<p>再来看看学习率的作用，如果学习率<script type="math/tex">\alpha</script>太小，结果就是函数值只能一点一点的挪动需要很多步才能到达全局最低点，那如果学习率<script type="math/tex">\alpha</script>太大，那么梯度下降就有可能会越过最低点，甚至可能无法收敛或发散</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220224212639223.png" alt="image-20220224212639223">在梯度下降法中，当我们接近局部最低点时，梯度下降法将会自动采取更小的幅度，这是因为我们越接近局部最低点，在该点处的导数值越小</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220224213434009.png" alt="image-20220224213434009"></p>
<h4 id="最小化平方差代价函数"><a href="#最小化平方差代价函数" class="headerlink" title="最小化平方差代价函数"></a>最小化平方差代价函数</h4><p>为了应用梯度下降法，关键的是导数项，我们将导数项化简</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)=\frac{\partial}{\partial\theta_j}\cdot\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2=\frac{\partial}{\partial\theta_j}\cdot\frac{1}{2m}\sum_{i=1}^m(\theta_0+\theta_1x^{(i)}-y^{(i)})^2</script><p>接下来求偏导</p>
<script type="math/tex; mode=display">
j=0:\frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\\j=1:\frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)=\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)}</script><p>由此得出线性回归算法</p>
<script type="math/tex; mode=display">
\rm{repeat}\ \rm{until}\ \rm{convergence}\ \{\\\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})\\\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})\cdot x^{(i)}\ \}</script><p>接下来，让我们来看看梯度下降是如何实现的，我们解决过的梯度下降问题之一是它容易陷入局部最优</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220306181054942.png" alt="image-20220306181054942" style="zoom:50%;" /></p>
<p>根据参数的不同初始值，梯度下降法会获得不同的极小值。然而，对于线性回归的代价函数来说，他的图像总是一个弓状函数（凹函数）。因此，它只有一个全局最优解。<strong>由此只要对线性回归的代价函数使用梯度下降法，它总是会收敛到全局最优</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220306181540173.png" alt="image-20220306181540173"></p>
<h2 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h2><p>之前我们学习了只有单一特征量的线性回归。假如，我们不只有一个特征量，而是有许多的特征量<script type="math/tex">x_1,x_2,...,x_n</script>，那么我们就要用到多变量线性回归</p>
<h3 id="假设函数"><a href="#假设函数" class="headerlink" title="假设函数"></a>假设函数</h3><p>多变量线性回归的<strong>假设函数</strong>为<script type="math/tex">h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n</script></p>
<p>为了标识方便我们要将<strong><script type="math/tex">x_0</script>的值设为1</strong>，这意味着对于第<script type="math/tex">i</script>个样本来说，有一个向量<script type="math/tex">x^{(i)}_0=1</script>，这样我们从有n个特征量变成了有n+1个特征量，由此<script type="math/tex">x=\begin{bmatrix}x_0\\x_1\\x_2\\\vdots\\x_n\end{bmatrix}\in\Re^{n+1}</script>，<script type="math/tex">\theta=\begin{bmatrix}\theta_0\\\theta_1\\\theta_2\\\vdots\\\theta_n\end{bmatrix}\in\Re^{n+1}</script>，<script type="math/tex">y=\begin{bmatrix}y_1\\y_2\\y_3\\\vdots\\y_n\end{bmatrix}</script>则上述假设函数满足：</p>
<script type="math/tex; mode=display">
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n=h_\theta(x)=\theta_0x_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n=\theta^Tx\\h_\theta(x)=\theta^Tx</script><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>我们不把<script type="math/tex">\theta_0,\theta_1,\cdots\theta_n</script>看成n+1个参数，而是把它们看作<script type="math/tex">\theta</script>这样一个n+1维向量</p>
<p>得出代价函数</p>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2m}\sum_{i=1}^m(\theta^Tx-y)^2</script><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>多变量线性回归的梯度下降函数</p>
<script type="math/tex; mode=display">
\rm{Repeat}\ \{ \\\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)\ \}\\</script><script type="math/tex; mode=display">
\rm{Repeat}\ \{\\\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})\cdot x^{(i)}_j\ \}</script><h3 id="特征缩放"><a href="#特征缩放" class="headerlink" title="特征缩放"></a>特征缩放</h3><p>假如我们有两个特征变量<script type="math/tex">x_1,x_2</script>，但<script type="math/tex">x_1=\rm{size}\in(0,2000)，x_2=\rm{num\ of\ bedroom}\in(1,5)</script>，这两个变量取值范围相差甚远，这时我们绘制出参数<script type="math/tex">\theta_1，\theta_2</script>与<script type="math/tex">J(\theta)</script>的等值线图</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220309100414308.png" alt="image-20220309100414308" style="zoom:50%;" /></p>
<p>我们会发现这个图像是一个很“椭”的椭圆，红线代表着梯度下降的过程，这个过程因来回震荡而十分的缓慢，而如果我们对特征变量进行缩放，即令<script type="math/tex">x_1=\frac{\rm{size}}{2000},x_2=\frac{\rm{num\ of\ bedroom}}{5}</script>，此时绘制出的图像就会比较“圆”</p>
<p><img src="https://cdn.jsdelivr.net/gh/Rfdfz/image/image-20220310140929602.png" alt="image-20220310140929602" style="zoom: 67%;" /></p>
<p>梯度下降的收敛速度变得更快，算法的效率得以提高</p>
<p>一般来说，特征缩放通常的目的是将特征的取值约束到<script type="math/tex">(-1,1)</script>这个区间中</p>
<p>在特征缩放的过程中，我们有时还会进行一个叫做<strong>均值归一化(Mean normalization)</strong>的操作，意思是，如果有一个特征<script type="math/tex">x_i</script>，你就用<script type="math/tex">x_i-\mu_i</script>来替换，其中的<script type="math/tex">\mu_i</script>为该特征值的平均值，使得你的特征值具有为0的平均值，很明显我们不需要把这一步作用在<script type="math/tex">x_0</script>上因为<script type="math/tex">x_0=1</script>它不可能具有值为0的平均值</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="单变量线性回归-1"><a href="#单变量线性回归-1" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h3><h4 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h4><p>我们知道单变量线性回归的代价函数表达式：</p>
<script type="math/tex; mode=display">
J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2</script><p>设<script type="math/tex">\Theta=\begin{bmatrix}\theta_0\\\theta_1\end{bmatrix}，X=\begin{bmatrix}1\ x^{(1)}\\1\ x^{(2)}\\\vdots\\1\ x^{(m)}\end{bmatrix}，y=\begin{bmatrix}y^{(1)}\\y^{(2)}\\\vdots\\y^{(m)}\end{bmatrix}</script>，则上式可化简为</p>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2m}\sum^m_{i=1}(X\cdot\Theta-y)^2</script><p>而我们知道其梯度下降函数的表达式：</p>
<script type="math/tex; mode=display">
\rm{repeat}\ \rm{until}\ \rm{convergence}\ \{\\\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})\\\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})\cdot x^{(i)}\ \}</script><p>我们引入一个新的参数<script type="math/tex">\delta=\begin{bmatrix}\frac{1}{m}\sum^m_{i=1}h_{\theta}(x^{(i)})-y^{(i)}\cdot x_0^{(i)}\\\frac{1}{m}\sum^m_{i=1}h_{\theta}(x^{(i)})-y^{(i)})\cdot x^{(i)}_1\end{bmatrix}=\frac{1}{m}\cdot(X\cdot\Theta-y)\left[\begin{array}{cccc}1&1&1&\cdots&1\\x_1^{(1)}&x_1^{(2)}&x_1^{(3)}&\cdots&x_1^{(m)}\end{array}\right]=\frac{1}{m}X^T\cdot(X\cdot\Theta-y)</script>，而上述方程可写作<script type="math/tex">\Theta=\Theta-\alpha\delta</script>，则可化简为</p>
<script type="math/tex; mode=display">
\rm{repeat}\ \rm{until}\ \rm{convergence}\ \{\\\Theta=\Theta-\alpha\frac{1}{m}X^T(X\Theta-y)\}</script><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据读取</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataLoad</span>(<span class="params">path</span>):</span></span><br><span class="line">    m_data = pd.read_csv(path, names=[<span class="string">&#x27;Population&#x27;</span>, <span class="string">&#x27;Profit&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> m_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建代价函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">costFunction</span>(<span class="params">theta, X, y</span>):</span></span><br><span class="line">    inner = np.power(np.dot(X, theta) - y, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(inner) / (<span class="number">2</span> * <span class="built_in">len</span>(X))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 梯度下降</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span>(<span class="params">X, y, theta, alpha, iters</span>):</span></span><br><span class="line">    costs = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">        cost = costFunction(theta, X, y)</span><br><span class="line">        theta = theta - (np.dot(X.T, (np.dot(X, theta) - y))) * alpha / <span class="built_in">len</span>(X)</span><br><span class="line">        costs.append(cost)</span><br><span class="line">    <span class="keyword">return</span> theta, costs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据初始化</span></span><br><span class="line">data = dataLoad(<span class="string">&#x27;ex1data1.txt&#x27;</span>)</span><br><span class="line">data.insert(<span class="number">0</span>, <span class="string">&#x27;Ones&#x27;</span>, <span class="number">1</span>)</span><br><span class="line">X = data.iloc[:, :-<span class="number">1</span>].values</span><br><span class="line">y = data.iloc[:, -<span class="number">1</span>].values</span><br><span class="line">y = y.reshape(<span class="number">97</span>, <span class="number">1</span>)</span><br><span class="line">theta = np.zeros((<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">alpha = <span class="number">0.02</span></span><br><span class="line">iters = <span class="number">10000</span></span><br><span class="line">theta, costs = gradientDescent(X, y, theta, alpha, iters)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;theta:\n<span class="subst">&#123;theta[<span class="number">0</span>], theta[<span class="number">1</span>]&#125;</span>\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;cost in each iters:&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> cost <span class="keyword">in</span> costs:</span><br><span class="line">    <span class="built_in">print</span>(cost)</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/02/22/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%8E%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/" rel="prev" title="监督学习与无监督学习">
      <i class="fa fa-chevron-left"></i> 监督学习与无监督学习
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/03/18/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/" rel="next" title="数据预处理">
      数据预处理 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-%E7%90%86%E8%AE%BA%E7%AF%87"><span class="nav-number">1.</span> <span class="nav-text">线性回归-理论篇</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.1.</span> <span class="nav-text">单变量线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="nav-number">1.1.1.</span> <span class="nav-text">监督学习的工作原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0-cost-function"><span class="nav-number">1.1.2.</span> <span class="nav-text">代价函数(cost function)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%AE%9A%E4%B9%89"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">代价函数的数学定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">如何理解代价函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">1.1.3.</span> <span class="nav-text">梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95%E7%9A%84%E6%95%B0%E5%AD%A6%E5%AE%9A%E4%B9%89"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">梯度下降算法的数学定义</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%8C%E6%AD%A5%E6%9B%B4%E6%96%B0"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">同步更新</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E7%90%86%E8%A7%A3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="nav-number">1.1.3.3.</span> <span class="nav-text">如何理解梯度下降法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%80%E5%B0%8F%E5%8C%96%E5%B9%B3%E6%96%B9%E5%B7%AE%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.3.4.</span> <span class="nav-text">最小化平方差代价函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.2.</span> <span class="nav-text">多变量线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%81%87%E8%AE%BE%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.1.</span> <span class="nav-text">假设函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">1.2.2.</span> <span class="nav-text">代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">1.2.3.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE"><span class="nav-number">1.2.4.</span> <span class="nav-text">特征缩放</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.3.</span> <span class="nav-text">代码实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%95%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92-1"><span class="nav-number">1.3.1.</span> <span class="nav-text">单变量线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E5%8C%96"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">向量化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">代码</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">林程星</p>
  <div class="site-description" itemprop="description">‘绳’和‘棒’一样，是人类最早创造的工具之一。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">25</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">林程星</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
      <span class="post-meta-item-text">站点总字数：</span>
    <span title="站点总字数">128k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    <span title="站点阅读时长">1:57</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>


        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
